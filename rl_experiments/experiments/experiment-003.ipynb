{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out how stacked LSTMs work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "\n",
    "# Add parent directory to Python path to import from sibling directories\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch  # noqa\n",
    "import torch.nn as nn  # noqa\n",
    "import torch.optim as optim  # noqa\n",
    "\n",
    "from a2c.convlstm import MultiLayerLinearLSTM  # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_overfit_multilayer_linearlstm():\n",
    "    \"\"\"\n",
    "    A simple test to ensure that the MultiLayerLinearLSTM can overfit\n",
    "    on a small synthetic sequence dataset -- but now using run_n_ticks()\n",
    "    for each time step in the sequence. Overfitting a small dataset\n",
    "    ensures that forward/backward passes are working as intended\n",
    "    and the model can eventually drive the loss near zero.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # HYPERPARAMETERS\n",
    "    # ----------------------------------------------------------------\n",
    "    input_dim = 10\n",
    "    hidden_dim = 8\n",
    "    num_layers = 2\n",
    "    seq_length = 5\n",
    "    batch_size = 4\n",
    "    lr = 1e-3\n",
    "    num_epochs = 5000\n",
    "\n",
    "    # Choose how many ticks to run per time step\n",
    "    n_ticks_per_step = 3  # Example: run 3 ticks each time we see a new input vector\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 1) CREATE A DUMMY \"SEQUENCE\" DATASET\n",
    "    # ----------------------------------------------------------------\n",
    "    # We'll generate random input sequences (batch_size, seq_length, input_dim)\n",
    "    # and want the model to predict the same sequence (identity mapping).\n",
    "    x_data = torch.randn(batch_size, seq_length, input_dim)\n",
    "    y_data = x_data.clone().detach()\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 2) DEFINE A SMALL MODEL THAT USES MultiLayerLinearLSTM\n",
    "    # ----------------------------------------------------------------\n",
    "    class OverfitModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "            super().__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            # Our custom multi-layer LSTM\n",
    "            self.rnn = MultiLayerLinearLSTM(input_dim, hidden_dim, num_layers)\n",
    "            # Final projection to get back to input_dim\n",
    "            self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        @property\n",
    "        def memory_size(self):\n",
    "            # For each layer, we have (c, h) = 2 * hidden_dim\n",
    "            return 2 * self.hidden_dim * self.num_layers\n",
    "\n",
    "        def forward(self, x, memory=None, n_ticks=1):\n",
    "            \"\"\"\n",
    "            x       : (batch, seq_length, input_dim)\n",
    "            memory  : (batch, 2 * hidden_dim * num_layers) if provided\n",
    "            n_ticks : number of times to run the LSTM per input\n",
    "            returns : (predictions, updated_memory),\n",
    "                      where predictions has shape (batch, seq_length, input_dim)\n",
    "            \"\"\"\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "\n",
    "            # If no memory is provided, initialize all c/h to zeros\n",
    "            if memory is None:\n",
    "                c_prev_list = [\n",
    "                    torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
    "                    for _ in range(self.num_layers)\n",
    "                ]\n",
    "                h_prev_list = [\n",
    "                    torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
    "                    for _ in range(self.num_layers)\n",
    "                ]\n",
    "            else:\n",
    "                c_prev_list, h_prev_list = self.unpack_memory(memory)\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            # For each time step in the sequence\n",
    "            for t in range(seq_len):\n",
    "                i_t = x[:, t, :]  # shape: (batch, input_dim)\n",
    "\n",
    "                # 3) Run the multi-layer LSTM for n_ticks for each i_t\n",
    "                #    Instead of a single forward pass, we do run_n_ticks.\n",
    "                #    This will repeatedly update c_prev_list/h_prev_list.\n",
    "                c_prev_list, h_prev_list = self.rnn.run_n_ticks(\n",
    "                    i_t, c_prev_list, h_prev_list, n_ticks=n_ticks\n",
    "                )\n",
    "\n",
    "                # The top-layer hidden state after run_n_ticks becomes our \"embedding\"\n",
    "                out_t = self.output_layer(h_prev_list[-1])  # (batch, input_dim)\n",
    "                outputs.append(out_t.unsqueeze(1))  # keep seq dimension\n",
    "\n",
    "            # Concatenate outputs along time dimension\n",
    "            outputs = torch.cat(outputs, dim=1)  # (batch, seq_length, input_dim)\n",
    "\n",
    "            # Repack the final states into a single memory tensor\n",
    "            new_memory = self.pack_memory(c_prev_list, h_prev_list)\n",
    "\n",
    "            return outputs, new_memory\n",
    "\n",
    "        def pack_memory(self, c_list, h_list):\n",
    "            \"\"\"\n",
    "            Convert [c_1, h_1, ..., c_D, h_D] into a single tensor of shape\n",
    "            (batch_size, 2 * hidden_dim * num_layers).\n",
    "            \"\"\"\n",
    "            new_memory = []\n",
    "            for d in range(self.num_layers):\n",
    "                new_memory.append(c_list[d])\n",
    "                new_memory.append(h_list[d])\n",
    "            return torch.cat(new_memory, dim=1)\n",
    "\n",
    "        def unpack_memory(self, memory):\n",
    "            \"\"\"\n",
    "            Reverse of pack_memory. Extract each layer's (c, h) from memory.\n",
    "            \"\"\"\n",
    "            c_prev_list = []\n",
    "            h_prev_list = []\n",
    "            slice_size = self.hidden_dim\n",
    "            for d in range(self.num_layers):\n",
    "                c_start = 2 * d * slice_size\n",
    "                h_start = c_start + slice_size\n",
    "                c_d = memory[:, c_start : c_start + slice_size]\n",
    "                h_d = memory[:, h_start : h_start + slice_size]\n",
    "                c_prev_list.append(c_d)\n",
    "                h_prev_list.append(h_d)\n",
    "            return c_prev_list, h_prev_list\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3) INITIALIZE MODEL, OPTIMIZER, LOSS\n",
    "    # ----------------------------------------------------------------\n",
    "    model = OverfitModel(input_dim, hidden_dim, num_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # We will keep track of memory across epochs if we like,\n",
    "    # or re-init each epoch. For demonstration we keep it persistent.\n",
    "    memory = None\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4) TRAINING LOOP TO OVERFIT\n",
    "    # ----------------------------------------------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (run_n_ticks is invoked inside model.forward for each step)\n",
    "        preds, memory = model(x_data, memory=memory, n_ticks=n_ticks_per_step)\n",
    "\n",
    "        # Compute MSE loss\n",
    "        loss = criterion(preds, y_data)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Optionally detach memory to avoid accumulating gradients through time\n",
    "        memory = memory.detach()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}\")\n",
    "\n",
    "    # Final check: after training, the loss should be close to zero\n",
    "    print(\"Final loss:\", loss.item())\n",
    "    assert loss.item() < 1e-3, (\n",
    "        \"Loss did not go below 1e-3. Overfitting test failed. \"\n",
    "        \"Check your LSTM or training logic.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Loss: 0.667332\n",
      "Epoch [200/5000], Loss: 0.416788\n",
      "Epoch [300/5000], Loss: 0.245756\n",
      "Epoch [400/5000], Loss: 0.156294\n",
      "Epoch [500/5000], Loss: 0.108760\n",
      "Epoch [600/5000], Loss: 0.076148\n",
      "Epoch [700/5000], Loss: 0.060032\n",
      "Epoch [800/5000], Loss: 0.048084\n",
      "Epoch [900/5000], Loss: 0.040887\n",
      "Epoch [1000/5000], Loss: 0.038212\n",
      "Epoch [1100/5000], Loss: 0.036612\n",
      "Epoch [1200/5000], Loss: 0.035683\n",
      "Epoch [1300/5000], Loss: 0.035018\n",
      "Epoch [1400/5000], Loss: 0.034926\n",
      "Epoch [1500/5000], Loss: 0.035180\n",
      "Epoch [1600/5000], Loss: 0.033926\n",
      "Epoch [1700/5000], Loss: 0.033326\n",
      "Epoch [1800/5000], Loss: 0.032906\n",
      "Epoch [1900/5000], Loss: 0.032651\n",
      "Epoch [2000/5000], Loss: 0.032485\n",
      "Epoch [2100/5000], Loss: 0.032367\n",
      "Epoch [2200/5000], Loss: 0.032274\n",
      "Epoch [2300/5000], Loss: 0.032194\n",
      "Epoch [2400/5000], Loss: 0.032124\n",
      "Epoch [2500/5000], Loss: 0.032060\n",
      "Epoch [2600/5000], Loss: 0.031969\n",
      "Epoch [2700/5000], Loss: 0.031887\n",
      "Epoch [2800/5000], Loss: 0.031827\n",
      "Epoch [2900/5000], Loss: 0.031812\n",
      "Epoch [3000/5000], Loss: 0.031834\n",
      "Epoch [3100/5000], Loss: 0.031720\n",
      "Epoch [3200/5000], Loss: 0.031691\n",
      "Epoch [3300/5000], Loss: 0.031674\n",
      "Epoch [3400/5000], Loss: 0.031687\n",
      "Epoch [3500/5000], Loss: 0.031956\n",
      "Epoch [3600/5000], Loss: 0.031736\n",
      "Epoch [3700/5000], Loss: 0.031653\n",
      "Epoch [3800/5000], Loss: 0.031634\n",
      "Epoch [3900/5000], Loss: 0.031631\n",
      "Epoch [4000/5000], Loss: 0.031630\n",
      "Epoch [4100/5000], Loss: 0.031629\n",
      "Epoch [4200/5000], Loss: 0.031629\n",
      "Epoch [4300/5000], Loss: 0.031629\n",
      "Epoch [4400/5000], Loss: 0.031629\n",
      "Epoch [4500/5000], Loss: 0.031629\n",
      "Epoch [4600/5000], Loss: 0.031629\n",
      "Epoch [4700/5000], Loss: 0.031630\n",
      "Epoch [4800/5000], Loss: 0.031649\n",
      "Epoch [4900/5000], Loss: 0.031632\n",
      "Epoch [5000/5000], Loss: 0.031630\n",
      "Final loss: 0.03162955492734909\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Loss did not go below 1e-3. Overfitting test failed. Check your LSTM or training logic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_overfit_multilayer_linearlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 162\u001b[0m, in \u001b[0;36mtest_overfit_multilayer_linearlstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Final check: after training, the loss should be close to zero\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-3\u001b[39m, (\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss did not go below 1e-3. Overfitting test failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck your LSTM or training logic.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Loss did not go below 1e-3. Overfitting test failed. Check your LSTM or training logic."
     ]
    }
   ],
   "source": [
    "test_overfit_multilayer_linearlstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
